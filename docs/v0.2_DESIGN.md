# AlphaPeptFast v0.2 Design - Fragment Indexing + Spectrum Search

**Library Version**: 0.2.0
**Purpose**: Core proteomics algorithms for fragment generation, spectrum search, and peptide databases
**Principle**: Battle-tested in production → Extract → Generalize → Document → Test

---

## Design Philosophy

### 1. **Battle-Tested Before Inclusion**
- All algorithms must work in production projects first
- Performance validated on real proteomics data
- No "nice to have" features - only proven essentials
- Each function extracted from successful project implementations

### 2. **Numba-First Performance**
- All hot paths JIT-compiled with `@numba.jit(nopython=True)`
- Use simple NumPy arrays, not complex objects
- Structured arrays for complex data (Numba-compatible)
- Target: >100k peptides/second for fragment generation, >1M fragments/second for matching

### 3. **Zero Heavy Dependencies**
- Core: NumPy + Numba + SciPy only
- No Pandas in hot paths (I/O only)
- No sklearn in library (applications can use it for scoring)
- No PyTorch/TensorFlow (prediction models are external)

### 4. **Separation of Concerns**
- **AlphaPeptFast**: Computational algorithms only
- **Projects**: Orchestration, I/O, machine learning, visualization
- **Clean API**: Functions take arrays, return arrays
- **No side effects**: No file I/O, no globals, no state

---

## Module 1: `alphapeptfast.fragments`

### Core Fragment Generation

```python
@numba.jit(nopython=True)
def generate_by_ions(
    peptide_ord: np.ndarray,  # Peptide as ord() array for speed
    precursor_charge: int,
    fragment_types: tuple = ('b', 'y'),
    fragment_charges: tuple = (1, 2),
    include_precursor: bool = False,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generate theoretical b/y fragment m/z values.

    This is the core fragment generation algorithm used by all proteomics
    search engines. Generates b-ions (N-terminal) and y-ions (C-terminal)
    with specified charge states.

    Parameters
    ----------
    peptide_ord : np.ndarray
        Peptide sequence as ord() values (uint8 array). Convert with:
        `np.array([ord(c) for c in peptide], dtype=np.uint8)`
    precursor_charge : int
        Precursor charge state (typically 2-4 for tryptic peptides)
    fragment_types : tuple of str
        Fragment types to generate ('b', 'y', 'a', 'c', 'z')
        Default: ('b', 'y') - most common for CID/HCD
    fragment_charges : tuple of int
        Charge states to generate for each fragment (typically 1, 2)
        Fragments with charge > fragment_position are skipped

    Returns
    -------
    fragment_mz : np.ndarray (float32)
        m/z values of all fragments
    fragment_type : np.ndarray (uint8)
        Fragment type: 0=b, 1=y, 2=a, 3=c, 4=z
    fragment_position : np.ndarray (uint8)
        Fragment position: 1 to n-1 (where n = peptide length)
    fragment_charge : np.ndarray (uint8)
        Fragment charge state

    Performance
    -----------
    >100,000 peptides/second on single CPU core (typical proteome search)

    Examples
    --------
    >>> peptide_ord = np.array([ord(c) for c in "PEPTIDE"], dtype=np.uint8)
    >>> mz, types, pos, charges = generate_by_ions(peptide_ord, precursor_charge=2)
    >>> # Returns ~24 fragments (6 positions × 2 types × 2 charges)

    Notes
    -----
    - Only generates fragments where charge <= position (physical constraint)
    - Uses exact amino acid masses from `alphapeptfast.mass`
    - Includes water loss in mass calculations (b/y ion formation)
    - Modifications should be applied to peptide_ord before calling
    """
```

### Fragment Selection (with predictions)

```python
def select_top_fragments(
    fragment_mz: np.ndarray,
    fragment_type: np.ndarray,
    fragment_position: np.ndarray,
    fragment_charge: np.ndarray,
    predicted_intensity: Optional[np.ndarray] = None,
    n_fragments: int = 12,
    prefer_high_mass: bool = True,
) -> np.ndarray:
    """Select most informative fragments for targeted search.

    Used to reduce search space by selecting only the most informative
    fragments per peptide. Modern search engines (DIA-NN, AlphaDIA) use
    6-12 fragments per peptide for fast, accurate identification.

    Parameters
    ----------
    fragment_mz, fragment_type, fragment_position, fragment_charge : np.ndarray
        Fragment arrays from generate_by_ions()
    predicted_intensity : np.ndarray, optional
        Predicted intensities from AlphaPeptDeep or similar
        If None, uses heuristic scoring only
    n_fragments : int
        Number of top fragments to select (typically 6-12)
    prefer_high_mass : bool
        Prefer fragments with m/z > 0.5 * precursor_mz
        High-mass fragments are more specific

    Returns
    -------
    indices : np.ndarray
        Indices of top N fragments (into input arrays)

    Scoring
    -------
    When predicted_intensity provided:
    - 40%: Predicted intensity
    - 30%: m/z ratio (>0.5 preferred)
    - 20%: Charge state (+1 preferred)
    - 10%: Fragment position (avoid termini)

    Without predictions (heuristic):
    - 50%: m/z ratio (high mass preferred)
    - 30%: Charge state (+1 preferred)
    - 20%: Fragment position (middle preferred)

    Performance
    -----------
    >500,000 peptides/second (not Numba, but not bottleneck)

    Examples
    --------
    >>> # With AlphaPeptDeep predictions
    >>> intensities = predict_intensities(peptide_seq)  # External
    >>> top_indices = select_top_fragments(mz, types, pos, charges,
    ...                                     predicted_intensity=intensities,
    ...                                     n_fragments=12)
    >>> top_mz = mz[top_indices]
    """
```

---

## Module 2: `alphapeptfast.search`

### Binary Search (Core Algorithm)

```python
@numba.jit(nopython=True)
def binary_search_mz(
    spectrum_mz: np.ndarray,  # MUST be sorted ascending!
    target_mz: float,
    tol_ppm: float,
) -> int:
    """Find closest match within PPM tolerance using binary search.

    This is the fundamental operation for all spectrum search algorithms.
    Used billions of times per proteome search, must be O(log n) fast.

    Parameters
    ----------
    spectrum_mz : np.ndarray (float32 or float64)
        Sorted m/z array from spectrum or feature list
        CRITICAL: Must be sorted ascending! No validation for speed.
    target_mz : float
        Theoretical fragment m/z to search for
    tol_ppm : float
        Mass tolerance in parts per million (typically 5-20 ppm)

    Returns
    -------
    index : int
        Index of closest match within tolerance
        Returns -1 if no match found

    Performance
    -----------
    >1,000,000 operations/second (O(log n) complexity)

    Examples
    --------
    >>> spectrum_mz = np.array([100.05, 200.10, 300.15, 400.20], dtype=np.float32)
    >>> idx = binary_search_mz(spectrum_mz, target_mz=200.11, tol_ppm=50)
    >>> # Returns 1 (matches 200.10 within 50 ppm)

    Notes
    -----
    - No bounds checking for performance - use only with sorted arrays
    - Returns closest match within tolerance, not all matches
    - For multiple matches, returns the closest one
    - PPM tolerance is: abs(observed - theoretical) / theoretical * 1e6
    """
```

### Fragment Matching (Main Search Function)

```python
@numba.jit(nopython=True)
def match_fragments_to_spectrum(
    theoretical_mz: np.ndarray,       # From generate_by_ions()
    theoretical_type: np.ndarray,     # 0=b, 1=y, etc.
    theoretical_position: np.ndarray,
    theoretical_charge: np.ndarray,
    spectrum_mz: np.ndarray,          # Sorted!
    spectrum_intensity: np.ndarray,
    mz_tol_ppm: float = 10.0,
    min_intensity: float = 0.0,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Match theoretical fragments to observed spectrum.

    Core algorithm for peptide spectrum matching. For each theoretical
    fragment, performs binary search to find matches in observed spectrum.
    Used in both DDA and DIA search engines.

    Parameters
    ----------
    theoretical_mz, theoretical_type, theoretical_position, theoretical_charge : np.ndarray
        Fragment arrays from generate_by_ions()
    spectrum_mz : np.ndarray (float32)
        Observed m/z values (MUST be sorted!)
    spectrum_intensity : np.ndarray (float32)
        Observed intensities (parallel to spectrum_mz)
    mz_tol_ppm : float
        Mass tolerance in PPM (typically 5-20 ppm)
    min_intensity : float
        Minimum intensity threshold for matches

    Returns
    -------
    match_indices : np.ndarray
        Indices into theoretical arrays for matched fragments
    observed_mz : np.ndarray
        Matched m/z values from spectrum
    observed_intensity : np.ndarray
        Matched intensities from spectrum
    mass_errors_ppm : np.ndarray
        Mass errors in PPM for each match

    Performance
    -----------
    >10,000 peptides/second with 100 fragments each = 1M fragments/second

    Examples
    --------
    >>> # Generate theoretical fragments
    >>> theo_mz, theo_type, theo_pos, theo_charge = generate_by_ions(peptide_ord, 2)
    >>>
    >>> # Match to spectrum
    >>> matches = match_fragments_to_spectrum(
    ...     theo_mz, theo_type, theo_pos, theo_charge,
    ...     spectrum_mz, spectrum_intensity,
    ...     mz_tol_ppm=10.0
    ... )
    >>> match_idx, obs_mz, obs_int, ppm_errors = matches
    >>>
    >>> # Calculate coverage
    >>> coverage = len(match_idx) / len(theo_mz)

    Notes
    -----
    - Each theoretical fragment matched at most once
    - If multiple spectrum peaks match, returns closest
    - Mass errors useful for calibration and quality assessment
    """
```

### Advanced Matching with RT and Ion Mirroring

```python
@numba.jit(nopython=True)
def match_fragments_with_coelution(
    theoretical_mz: np.ndarray,
    theoretical_type: np.ndarray,
    theoretical_position: np.ndarray,
    theoretical_charge: np.ndarray,
    spectrum_mz: np.ndarray,
    spectrum_intensity: np.ndarray,
    spectrum_rt: np.ndarray,          # New: RT for each feature
    precursor_rt: float,              # Expected precursor RT
    precursor_mass: float,            # For ion mirroring
    mz_tol_ppm: float = 10.0,
    rt_tol_sec: float = 3.0,
    enable_ion_mirror: bool = True,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Match fragments with RT coelution and ion mirroring for modifications.

    Extended matching algorithm for feature-based search. Requires fragments
    to co-elute with precursor (RT constraint) and optionally searches for
    complementary fragments to detect modifications.

    Parameters
    ----------
    theoretical_mz, ... : np.ndarray
        Same as match_fragments_to_spectrum()
    spectrum_rt : np.ndarray (float32)
        RT for each feature in spectrum (parallel to spectrum_mz)
    precursor_rt : float
        Expected precursor retention time in seconds
    precursor_mass : float
        Precursor neutral mass for ion mirroring calculation
    rt_tol_sec : float
        RT tolerance in seconds (typically 3-10 sec for DIA)
    enable_ion_mirror : bool
        Whether to search for complementary fragments
        If True, for each b-ion also searches for complementary y-ion

    Returns
    -------
    match_indices : np.ndarray
        Indices of matched theoretical fragments
    observed_mz : np.ndarray
        Observed m/z values
    observed_intensity : np.ndarray
        Observed intensities
    mass_shifts : np.ndarray
        Mass shifts from theoretical (for modification detection)
        Calculated via ion mirroring: precursor_mass - observed_mass
    rt_deltas : np.ndarray
        RT differences from precursor_rt (for quality assessment)

    Performance
    -----------
    >10,000 peptides/second (same as basic matching)

    Ion Mirroring Concept
    ---------------------
    For peptide ABCD with mass M:
    - b3 fragment = ABC (mass = m_b3)
    - y1 fragment = D (mass = M - m_b3)
    - If we observe b3 at m_observed, complementary fragment should be at M - m_observed
    - If actual complementary fragment at M - m_observed + delta, then modification = delta

    This enables modification-aware search without enumerating all possible modifications!

    Examples
    --------
    >>> matches = match_fragments_with_coelution(
    ...     theo_mz, theo_type, theo_pos, theo_charge,
    ...     feature_mz, feature_intensity, feature_rt,
    ...     precursor_rt=450.5,
    ...     precursor_mass=1500.75,
    ...     mz_tol_ppm=10.0,
    ...     rt_tol_sec=5.0,
    ...     enable_ion_mirror=True
    ... )
    >>> match_idx, obs_mz, obs_int, mass_shifts, rt_delta = matches
    >>>
    >>> # Check for modifications
    >>> potential_mods = mass_shifts[np.abs(mass_shifts) > 0.5]  # >0.5 Da shift
    """
```

### Ion Mirroring Utilities

```python
@numba.jit(nopython=True)
def calculate_complementary_mz(
    precursor_mass: float,
    fragment_mz: np.ndarray,
    fragment_charge: np.ndarray,
) -> np.ndarray:
    """Calculate complementary fragment m/z values for ion mirroring.

    For each fragment, calculates what m/z the complementary fragment
    should have based on precursor mass conservation.

    Parameters
    ----------
    precursor_mass : float
        Neutral mass of precursor peptide
    fragment_mz : np.ndarray
        m/z values of fragments
    fragment_charge : np.ndarray
        Charge states of fragments

    Returns
    -------
    complementary_mz : np.ndarray
        Expected m/z values of complementary fragments
        Calculated as: (precursor_mass - fragment_neutral_mass + H * charge) / charge

    Examples
    --------
    >>> # For peptide with mass 1500 Da, charge 2+
    >>> frag_mz = np.array([200.0, 300.0, 400.0])
    >>> frag_charge = np.array([1, 1, 1])
    >>> comp_mz = calculate_complementary_mz(1500.0, frag_mz, frag_charge)
    >>> # Returns [1301.0, 1201.0, 1101.0] (complementary fragments)
    """
```

---

## Module 3: `alphapeptfast.database`

### Peptide Database with Mass Index

```python
class PeptideDatabase:
    """Fast peptide database with binary-searchable mass index.

    Central data structure for proteome-scale peptide search. Stores peptides
    sorted by neutral mass for O(log n) candidate selection.

    Attributes
    ----------
    peptides : list of str
        All peptide sequences
    neutral_masses : np.ndarray (float64)
        Neutral masses, sorted ascending
    sort_indices : np.ndarray (int32)
        Original indices (before sorting)
        peptides[sort_indices[i]] corresponds to neutral_masses[i]

    Design Principles
    -----------------
    1. Mass-sorted for binary search (O(log n) candidate selection)
    2. Minimal memory footprint (strings + 12 bytes per peptide)
    3. No protein information in core database (separate mapping)
    4. Thread-safe (read-only after construction)

    Examples
    --------
    >>> # Create from peptide list
    >>> peptides = ["PEPTIDE", "SEQUENCE", "PROTEIN"]
    >>> db = PeptideDatabase(peptides)
    >>>
    >>> # Search by mass
    >>> candidates = db.search_by_mass(mass=850.4, charge=2, tol_ppm=5.0)
    >>> print([db.peptides[i] for i in candidates])
    """

    def __init__(self, peptides: List[str], modifications: Optional[Dict[str, float]] = None):
        """Build database with mass index.

        Parameters
        ----------
        peptides : list of str
            Peptide sequences (any order)
        modifications : dict, optional
            Fixed modifications: {'C': 57.021464}  # Carbamidomethyl
        """

    @staticmethod
    @numba.jit(nopython=True)
    def search_mass_range_numba(
        masses: np.ndarray,      # Sorted!
        target_mass: float,
        tol_ppm: float,
    ) -> tuple[int, int]:
        """Binary search for peptide mass range (Numba implementation).

        Parameters
        ----------
        masses : np.ndarray (float64)
            Sorted neutral masses
        target_mass : float
            Target neutral mass
        tol_ppm : float
            Tolerance in PPM

        Returns
        -------
        start_idx : int
            First index in range
        end_idx : int
            Last index in range (exclusive)

        Performance
        -----------
        O(log n + k) where k = number of matches
        >1,000,000 queries/second

        Examples
        --------
        >>> masses = np.array([100.0, 200.0, 300.0, 400.0])
        >>> start, end = PeptideDatabase.search_mass_range_numba(masses, 200.0, 50.0)
        >>> # Returns (1, 2) - one match at index 1
        """

    def search_by_mass(
        self,
        mass: float,
        charge: int,
        tol_ppm: float = 5.0,
    ) -> np.ndarray:
        """Get candidate peptide indices at given mass.

        Parameters
        ----------
        mass : float
            Precursor m/z or neutral mass
        charge : int
            Precursor charge (if mass is m/z, converts to neutral mass)
        tol_ppm : float
            Mass tolerance in PPM

        Returns
        -------
        indices : np.ndarray
            Indices into self.peptides array

        Examples
        --------
        >>> indices = db.search_by_mass(mass=850.4, charge=2, tol_ppm=5.0)
        >>> candidates = [db.peptides[i] for i in indices]
        """

    def search_by_mz(
        self,
        mz: float,
        charge: int,
        tol_ppm: float = 5.0,
    ) -> np.ndarray:
        """Get candidate peptides at given m/z (convenience method)."""

    @classmethod
    def from_fasta(
        cls,
        fasta_path: str,
        enzyme: str = 'trypsin',
        missed_cleavages: int = 0,
        min_length: int = 6,
        max_length: int = 30,
        modifications: Optional[Dict[str, float]] = None,
    ) -> 'PeptideDatabase':
        """Build database from FASTA file with in silico digestion.

        Parameters
        ----------
        fasta_path : str
            Path to FASTA file
        enzyme : str
            Enzyme name ('trypsin', 'lysc', 'argc', etc.)
        missed_cleavages : int
            Number of missed cleavages allowed (0-2 typical)
        min_length, max_length : int
            Peptide length constraints
        modifications : dict
            Fixed modifications

        Returns
        -------
        database : PeptideDatabase
            Database with digested peptides

        Examples
        --------
        >>> db = PeptideDatabase.from_fasta(
        ...     'uniprot_human.fasta',
        ...     enzyme='trypsin',
        ...     missed_cleavages=1,
        ...     modifications={'C': 57.021464}
        ... )
        """
```

### Target-Decoy Database

```python
class TargetDecoyDatabase(PeptideDatabase):
    """Peptide database with integrated target-decoy approach for FDR control.

    Extends PeptideDatabase with decoy sequences for false discovery rate
    estimation. Decoys generated by sequence reversal (preserving C-terminal
    K/R for trypsin).

    Attributes
    ----------
    n_targets : int
        Number of target peptides
    n_decoys : int
        Number of decoy peptides
    is_decoy : np.ndarray (bool)
        Mask indicating decoy peptides

    Organization
    ------------
    - Indices 0 to n_targets-1: Target peptides
    - Indices n_targets to end: Decoy peptides
    - Both sorted by mass (targets and decoys interleaved)

    Examples
    --------
    >>> db = TargetDecoyDatabase.from_fasta('uniprot_human.fasta')
    >>> print(f"Targets: {db.n_targets}, Decoys: {db.n_decoys}")
    >>>
    >>> # Check if peptide is decoy
    >>> idx = 12345
    >>> if db.is_decoy[idx]:
    ...     print("This is a decoy")
    """

    def __init__(self, target_peptides: List[str], **kwargs):
        """Build target-decoy database.

        Automatically generates decoys by reversing target sequences.
        """

    @staticmethod
    def reverse_peptide(peptide: str, preserve_terminal: bool = True) -> str:
        """Generate decoy by reversing sequence.

        Parameters
        ----------
        peptide : str
            Target peptide sequence
        preserve_terminal : bool
            Preserve C-terminal residue (for trypsin)

        Returns
        -------
        decoy : str
            Reversed peptide sequence

        Examples
        --------
        >>> reverse_peptide("PEPTIDER", preserve_terminal=True)
        'EDITPEPR'  # R preserved at C-terminus
        """
```

---

## Module 4: `alphapeptfast.mass` (Extensions)

### Mass Calculations

```python
# Amino acid masses (monoisotopic)
AA_MASSES = {
    'A': 71.037114, 'R': 156.101111, 'N': 114.042927,
    'D': 115.026943, 'C': 103.009185, 'E': 129.042593,
    'Q': 128.058578, 'G': 57.021464, 'H': 137.058912,
    'I': 113.084064, 'L': 113.084064, 'K': 128.094963,
    'M': 131.040485, 'F': 147.068414, 'P': 97.052764,
    'S': 87.032028, 'T': 101.047679, 'W': 186.079313,
    'Y': 163.063320, 'V': 99.068414
}

# Constants
PROTON_MASS = 1.007825
H2O_MASS = 18.010565
NH3_MASS = 17.026549
CO_MASS = 27.994915

@numba.jit(nopython=True)
def calculate_neutral_mass(
    peptide_ord: np.ndarray,
    modifications: Optional[np.ndarray] = None,
) -> float:
    """Calculate neutral peptide mass from ord() array.

    Parameters
    ----------
    peptide_ord : np.ndarray (uint8)
        Peptide as ord() values
    modifications : np.ndarray (float32), optional
        Modification masses at each position
        Same length as peptide_ord

    Returns
    -------
    mass : float
        Neutral peptide mass (includes terminal H2O)

    Examples
    --------
    >>> peptide_ord = np.array([ord(c) for c in "PEPTIDE"], dtype=np.uint8)
    >>> mass = calculate_neutral_mass(peptide_ord)
    """

@numba.jit(nopython=True)
def calculate_precursor_mz(
    neutral_mass: float,
    charge: int,
) -> float:
    """Calculate precursor m/z from neutral mass.

    Formula: (neutral_mass + charge * PROTON_MASS) / charge

    Examples
    --------
    >>> mz = calculate_precursor_mz(1000.5, charge=2)
    >>> # Returns 501.26 (approximately)
    """

@numba.jit(nopython=True)
def calculate_fragment_mz(
    fragment_mass: float,
    charge: int,
) -> float:
    """Calculate fragment m/z from neutral mass."""

@numba.jit(nopython=True)
def ppm_error(
    observed_mz: float,
    theoretical_mz: float,
) -> float:
    """Calculate mass error in PPM.

    Formula: (observed - theoretical) / theoretical * 1e6

    Examples
    --------
    >>> error = ppm_error(observed=500.1, theoretical=500.0)
    >>> # Returns 200.0 ppm
    """
```

---

## Module 5: `alphapeptfast.utils` (Extensions)

### Data Structures

```python
# Use NumPy structured arrays (Numba-compatible!)

# Fragment match result
MATCH_DTYPE = np.dtype([
    ('fragment_type', np.uint8),     # 0=b, 1=y, 2=a, 3=c, 4=z
    ('position', np.uint8),          # 1 to n-1
    ('charge', np.uint8),            # Fragment charge
    ('theoretical_mz', np.float32),
    ('observed_mz', np.float32),
    ('intensity', np.float32),
    ('mass_error_ppm', np.float32),
])

def create_match_array(n_matches: int) -> np.ndarray:
    """Create empty match array with MATCH_DTYPE structure."""
    return np.zeros(n_matches, dtype=MATCH_DTYPE)

# Extended match with RT and mass shifts
EXTENDED_MATCH_DTYPE = np.dtype([
    ('fragment_type', np.uint8),
    ('position', np.uint8),
    ('charge', np.uint8),
    ('theoretical_mz', np.float32),
    ('observed_mz', np.float32),
    ('intensity', np.float32),
    ('rt', np.float32),              # Retention time
    ('mass_shift', np.float32),      # From ion mirroring
    ('mass_error_ppm', np.float32),
])

# Peptide candidate result
CANDIDATE_DTYPE = np.dtype([
    ('peptide_idx', np.int32),       # Index in database
    ('precursor_mz', np.float32),
    ('charge', np.uint8),
    ('n_matched', np.uint16),        # Number of matched fragments
    ('score', np.float32),           # Simple score
])
```

---

## Implementation Strategy

### Phase 0: Battle-Test in Projects

**Before adding ANY function to AlphaPeptFast:**

1. Implement in project (e.g., ProteinFirst, AlphaMod, etc.)
2. Test on real proteomics data (millions of spectra)
3. Validate results against ground truth
4. Measure performance (must meet targets)
5. Iterate until proven

**Quality Gates:**
- ✅ Works on full-scale data (not toy examples)
- ✅ Performance meets targets (>100k ops/sec typical)
- ✅ Results validated (comparison to AlphaDIA, Mascot, etc.)
- ✅ Code reviewed and cleaned

### Phase 1: Extract to AlphaPeptFast

**When extracting from project:**

1. **Generalize**: Remove project-specific code
   - No hardcoded paths
   - No I/O in core functions
   - No business logic (scoring, FDR, etc.)
   - Pure computational algorithms only

2. **Document**: Comprehensive NumPy-style docstrings
   ```python
   def function(param1: type, param2: type) -> return_type:
       """One-line summary.

       Detailed description of algorithm.
       Explain non-obvious design decisions.

       Parameters
       ----------
       param1 : type
           Description

       Returns
       -------
       return_type
           Description

       Performance
       -----------
       >100k operations/second on typical data

       Examples
       --------
       >>> result = function(arg1, arg2)

       Notes
       -----
       Important caveats, limitations, or assumptions.
       """
   ```

3. **Test**: Unit tests + integration tests
   - Unit tests with toy data (fast, specific)
   - Integration tests with real data (slow, comprehensive)
   - Performance benchmarks tracked over time
   - Target: >90% code coverage

4. **Performance**: Benchmark and optimize
   - Profile with real data
   - Optimize hot paths with Numba
   - Document performance characteristics
   - Add regression tests

5. **Commit**: Clear provenance
   ```
   git commit -m "feat: add fragment generation from ProteinFirst v1.2

   Battle-tested on 271k precursors, 106M fragments.
   Performance: 120k peptides/sec on M1 Max.
   Validation: 95% match with AlphaDIA on 47k ground truth peptides.

   Extracted from: ProteinFirst_MS1centric/src/proteinfirst/fragments.py
   Original commit: a1b2c3d
   "
   ```

### Phase 2: Use from Projects

**After extraction:**

1. Install AlphaPeptFast in project: `pip install -e ../AlphaPeptFast`
2. Replace local implementation with library import
3. Keep only glue code in project (orchestration, I/O, visualization)
4. Project becomes lightweight consumer of library

**Example project structure after extraction:**

```
ProteinFirst_MS1centric/
├── scripts/
│   └── run_dia_search.py        # Main script (uses AlphaPeptFast)
│   └── validate_results.py      # Project-specific validation
├── src/proteinfirst/            # Only project-specific code
│   ├── scoring/                 # RF/NN scoring (not general enough)
│   ├── visualization/           # Project-specific plots
│   └── validation/              # AlphaDIA comparison (specific)
├── data/                        # Data files
└── tests/                       # Project-specific tests
```

---

## Design Principles Summary

### 1. **Separation of Concerns**

```python
# ✅ Good: Pure computation in library
from alphapeptfast.search import match_fragments_to_spectrum

def search_spectrum(spectrum_data, peptide):
    matches = match_fragments_to_spectrum(
        theoretical_mz=peptide.fragments,
        spectrum_mz=spectrum_data.mz,
        ...
    )
    return matches

# ❌ Bad: I/O and computation mixed
def search_spectrum(spectrum_file, peptide_file):
    # Don't load files in library functions!
    spectrum = load_mzml(spectrum_file)
    peptides = load_fasta(peptide_file)
    ...
```

### 2. **Numba-Compatible Data Structures**

```python
# ✅ Good: Simple arrays and structured arrays
@numba.jit(nopython=True)
def process_matches(matches: np.ndarray):  # MATCH_DTYPE structured array
    return matches[matches['intensity'] > 1000]

# ❌ Bad: Complex objects (not Numba-compatible)
class Match:
    def __init__(self, mz, intensity):
        self.mz = mz
        self.intensity = intensity

def process_matches(matches: List[Match]):  # Can't use Numba!
    ...
```

### 3. **Performance First**

```python
# ✅ Good: Pre-allocated arrays, Numba JIT
@numba.jit(nopython=True)
def generate_fragments(peptide_ord, charge):
    n_fragments = (len(peptide_ord) - 1) * 2 * 2  # Pre-calculate
    mz = np.empty(n_fragments, dtype=np.float32)  # Pre-allocate
    # ... fill array ...
    return mz

# ❌ Bad: Dynamic lists, no JIT
def generate_fragments(peptide, charge):
    fragments = []  # Dynamic allocation!
    for i in range(len(peptide)):
        fragments.append(calculate_mz(peptide[:i]))  # Slow!
    return fragments
```

### 4. **Battle-Tested Only**

```python
# ✅ Good: Proven in production
# Commit message:
"""
feat: add core-anneal feature finding

Battle-tested on 300 windows, 106M features, 7 minutes total.
Compared to baseline: 25% more high-quality features.
Validation: MS1 precursor recovery 71.4% on AlphaDIA ground truth.

Extracted from: ProteinFirst/src/features/core_anneal_finder.py
"""

# ❌ Bad: Nice idea, not tested
# Commit message:
"""
feat: add ML-based feature scoring

Might improve feature quality? Haven't tested on real data yet.
Based on paper XYZ, seems promising.
"""
```

---

## Success Criteria

### AlphaPeptFast v0.2 Ready When:

**Code Quality:**
- [ ] All modules have >90% test coverage
- [ ] All functions have NumPy-style docstrings
- [ ] Performance benchmarks documented
- [ ] No dependencies beyond NumPy + Numba + SciPy

**Performance:**
- [ ] Fragment generation: >100k peptides/second
- [ ] Binary search: >1M operations/second
- [ ] Fragment matching: >10k peptides/second (×100 fragments)
- [ ] Database search: <1ms per query

**Validation:**
- [ ] Works in at least one production project (ProteinFirst)
- [ ] Validated against ground truth (AlphaDIA, Mascot, etc.)
- [ ] Results reproducible across platforms
- [ ] Performance consistent across different data

**Documentation:**
- [ ] README updated with v0.2 examples
- [ ] API reference complete
- [ ] Migration guide from v0.1
- [ ] Performance characteristics documented

**Distribution:**
- [ ] pip installable from GitHub
- [ ] Version tagged (v0.2.0)
- [ ] Changelog updated
- [ ] Tests pass on CI/CD

### Project Using AlphaPeptFast is Clean When:

**Code Organization:**
- [ ] No core algorithms duplicated (uses library)
- [ ] Main script <300 lines (orchestration only)
- [ ] Separation of concerns (compute vs I/O vs ML)
- [ ] Clear dependency on AlphaPeptFast version

**Testing:**
- [ ] Can `pip install alphapeptfast` and import
- [ ] Project tests pass with library version
- [ ] Performance comparable to local implementation
- [ ] Results identical (bit-for-bit)

**Documentation:**
- [ ] Clear provenance (which functions from library)
- [ ] Project-specific logic documented
- [ ] AlphaPeptFast version pinned in requirements

---

## Example Usage (v0.2)

```python
# In any proteomics project
from alphapeptfast.fragments import generate_by_ions, select_top_fragments
from alphapeptfast.search import match_fragments_to_spectrum
from alphapeptfast.database import PeptideDatabase
from alphapeptfast.mass import calculate_neutral_mass

# Load peptide database (once)
db = PeptideDatabase.from_fasta(
    'uniprot_human.fasta',
    enzyme='trypsin',
    missed_cleavages=1,
    modifications={'C': 57.021464}  # Carbamidomethyl
)

# Search one spectrum
def search_one_spectrum(spectrum_mz, spectrum_int, precursor_mz, precursor_charge):
    """Project-specific: search spectrum against database."""

    # Get candidate peptides from database (library function)
    candidate_indices = db.search_by_mz(
        mz=precursor_mz,
        charge=precursor_charge,
        tol_ppm=5.0
    )

    results = []
    for idx in candidate_indices:
        peptide = db.peptides[idx]
        peptide_ord = np.array([ord(c) for c in peptide], dtype=np.uint8)

        # Generate fragments (library function)
        frag_mz, frag_type, frag_pos, frag_charge = generate_by_ions(
            peptide_ord, precursor_charge
        )

        # Match to spectrum (library function)
        match_idx, obs_mz, obs_int, ppm_errors = match_fragments_to_spectrum(
            frag_mz, frag_type, frag_pos, frag_charge,
            spectrum_mz, spectrum_int,
            mz_tol_ppm=10.0
        )

        # Score (project-specific - use your own method)
        score = calculate_my_score(match_idx, obs_int, peptide)

        results.append({
            'peptide': peptide,
            'score': score,
            'n_matched': len(match_idx),
        })

    # Return best match
    return max(results, key=lambda x: x['score'])
```

**Clean separation**: Library does computation, project does orchestration!

---

## Contributing

### Before Contributing a New Function:

1. **Check if it's general enough**
   - Would 3+ proteomics projects use this?
   - Is it pure computation (no I/O, no visualization)?
   - Can it be Numba-compiled?

2. **Battle-test it first**
   - Implement in your project
   - Test on real data (>1M operations)
   - Measure performance
   - Validate results

3. **Extract and document**
   - Remove project-specific code
   - Add comprehensive docstrings
   - Write unit tests + integration tests
   - Benchmark performance

4. **Submit PR with evidence**
   - Link to project where tested
   - Show performance benchmarks
   - Include validation results
   - Provide example usage

### Not Suitable for AlphaPeptFast:

❌ **I/O operations** - Projects handle their own data formats
❌ **Visualization** - Use Plotly, Matplotlib in projects
❌ **Machine learning models** - Projects use sklearn, PyTorch
❌ **Business logic** - FDR control, score thresholds, etc.
❌ **Project-specific workflows** - Orchestration stays in projects

✅ **Suitable**:
- Pure computational algorithms
- Numba-compilable functions
- Reusable across many projects
- Performance-critical operations
- Battle-tested in production

---

## Roadmap

### v0.2 (Current) - Fragment Search Foundation
- [x] Fragment generation (b/y ions)
- [ ] Binary search on sorted arrays
- [ ] Fragment matching with mass tolerance
- [ ] Ion mirroring for modification detection
- [ ] Peptide database with mass index
- [ ] Target-decoy database for FDR

### v0.3 (Future) - Advanced Search
- [ ] Neutral loss prediction
- [ ] Internal fragment generation
- [ ] Open modification search
- [ ] PTM localization scoring
- [ ] Fragment intensity prediction integration

### v0.4 (Future) - Performance Optimization
- [ ] GPU acceleration (CUDA, Metal)
- [ ] Mojo implementations for hot paths
- [ ] Memory-mapped databases
- [ ] Distributed search (multi-node)

---

## Contact and Resources

- **GitHub**: github.com/MannLabs/AlphaPeptFast
- **Documentation**: Full API docs (to be added)
- **Issues**: Report bugs, request features
- **Projects using AlphaPeptFast**:
  - ProteinFirst_MS1centric (feature-based DIA search)
  - (More to come)

**Principle**: This library grows through real-world use, not speculation!
